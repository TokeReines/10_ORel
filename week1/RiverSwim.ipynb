{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated V^pi via Monte Carlo:  [1.65185151 1.93620515 3.14803208 7.30577418 8.8869764 ]\n",
      "Exact V^pi:  [1.60365251 1.89408739 3.15492297 7.29485928 8.77220123]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Riverswim:\n",
    "    \"\"\"\n",
    "    Sourced from Absalon and rewritten by bkx591\n",
    "    \"\"\"\n",
    "    def __init__(self, states_count, actions_count=2, gamma=0.95):\n",
    "        self.states_count = states_count\n",
    "        self.actions_count = actions_count\n",
    "        self.P = np.zeros((states_count, actions_count, states_count))\n",
    "        self.R = np.zeros((states_count, actions_count))\n",
    "        self.gamma = gamma  # Discount factor\n",
    "\n",
    "        for s in range(states_count):\n",
    "            if s == 0:\n",
    "                self.P[s, 0, s] = 1\n",
    "                self.P[s, 1, s] = 0.6\n",
    "                self.P[s, 1, s + 1] = 0.4\n",
    "                self.R[s, 0] = 0.05\n",
    "            elif s == states_count - 1:\n",
    "                self.P[s, 0, s - 1] = 1\n",
    "                self.P[s, 1, s] = 0.6\n",
    "                self.P[s, 1, s - 1] = 0.4\n",
    "                self.R[s, 1] = 1 \n",
    "            else:\n",
    "                self.P[s, 0, s - 1] = 1\n",
    "                self.P[s, 1, s] = 0.55\n",
    "                self.P[s, 1, s + 1] = 0.4\n",
    "                self.P[s, 1, s - 1] = 0.05\n",
    "\n",
    "        self.s = 0\n",
    "\n",
    "    def reset(self, s=0):\n",
    "        self.s = s\n",
    "        return self.s\n",
    "\n",
    "    def step(self, action):\n",
    "        new_s = np.random.choice(np.arange(self.states_count), p=self.P[self.s, action])\n",
    "        reward = self.R[self.s, action]\n",
    "        self.s = new_s\n",
    "        return new_s, reward\n",
    "\n",
    "\n",
    "    def policy(self, s):\n",
    "        # Define the policy function based on the state\n",
    "        if s < 3:  # For states s1, s2, s3\n",
    "            return np.array([0.5, 0.5])  # Equal probabilities for 'left' and 'right'\n",
    "        else:  # For states s4, s5\n",
    "            return np.array([0.0, 1.0])  # Always 'right'\n",
    "\n",
    "    def policy_matrix(self):\n",
    "        # Create a policy matrix that represents the transition probabilities under the given policy.\n",
    "        policy_mat = np.zeros((self.states_count, self.states_count))\n",
    "        for s in range(self.states_count):\n",
    "            policy_prob = self.policy(s)\n",
    "            # Calculate the expected transition probability for each state under the given policy\n",
    "            policy_mat[s] = policy_prob[0] * self.P[s, 0] + policy_prob[1] * self.P[s, 1]\n",
    "        return policy_mat\n",
    "\n",
    "    def reward_vector(self):\n",
    "        # Create a reward vector that represents the expected immediate reward from each state under the given policy.\n",
    "        reward_vec = np.zeros(self.states_count)\n",
    "        for s in range(self.states_count):\n",
    "            policy_prob = self.policy(s)\n",
    "            # Calculate the expected immediate reward for each state under the given policy\n",
    "            reward_vec[s] = policy_prob[0] * self.R[s, 0] + policy_prob[1] * self.R[s, 1]\n",
    "        return reward_vec\n",
    "\n",
    "def monte_carlo_policy_evaluation(env, n, T):\n",
    "    V_pi = np.zeros(env.states_count)\n",
    "    for s in range(env.states_count):\n",
    "        G_sum = np.zeros(n)  # To store the sum of returns for each trajectory\n",
    "        for i in range(n):\n",
    "            state = env.reset(s)  # Start at the given state\n",
    "            G = 0  # Initialize return\n",
    "            for t in range(T):\n",
    "                action_probs = env.policy(state)\n",
    "                action = np.random.choice(env.actions_count, p=action_probs)\n",
    "                state, reward = env.step(action)\n",
    "                G += reward * (env.gamma ** t)\n",
    "            G_sum[i] = G\n",
    "        V_pi[s] = np.mean(G_sum)  # Calculate the average return for the state\n",
    "    return V_pi\n",
    "\n",
    "def compute_exact_v_pi(env):\n",
    "    P_pi = env.policy_matrix()\n",
    "    r_pi = env.reward_vector()\n",
    "    I = np.eye(env.states_count)\n",
    "    V_pi = np.linalg.inv(I - env.gamma * P_pi).dot(r_pi)\n",
    "    return V_pi\n",
    "\n",
    "# Create the environment\n",
    "env = Riverswim(5)\n",
    "\n",
    "# Run the Monte Carlo Policy Evaluation\n",
    "V_pi_estimate = monte_carlo_policy_evaluation(env, n=100, T=200)\n",
    "print(\"Estimated V^pi via Monte Carlo: \", V_pi_estimate)\n",
    "\n",
    "# Compute the exact V^pi\n",
    "V_pi_exact = compute_exact_v_pi(env)\n",
    "print(\"Exact V^pi: \", V_pi_exact)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10-orel-EypsHRW3-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
